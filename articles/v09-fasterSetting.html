<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Tips for faster computation • abess</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/cerulean/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><!-- docsearch --><script src="../docsearch.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/docsearch.js/2.6.3/docsearch.min.css" integrity="sha256-QOSRU/ra9ActyXkIBbiIB144aDBdtvXBcNc3OTNuX/Q=" crossorigin="anonymous">
<link href="../docsearch.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script><meta property="og:title" content="Tips for faster computation">
<meta property="og:description" content="abess">
<meta property="og:image" content="https://abess-team.github.io/abess/logo.svg">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">abess</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.4.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/v01-abess-guide.html">Quick start for `abess`: Linear regression</a>
    </li>
    <li>
      <a href="../articles/v03-classification.html">Classification: Logistic Regression and Multinomial Extension</a>
    </li>
    <li>
      <a href="../articles/v04-PoissonGammaReg.html">Positive response: Poisson and Gamma regression</a>
    </li>
    <li>
      <a href="../articles/v05-coxreg.html">Best Subset Selection for Censored Response</a>
    </li>
    <li>
      <a href="../articles/v06-MultiTaskLearning.html">Multi-Response Linear Regression</a>
    </li>
    <li>
      <a href="../articles/v07-advancedFeatures.html">Advanced Features</a>
    </li>
    <li>
      <a href="../articles/v08-sPCA.html">Principal component analysis</a>
    </li>
    <li>
      <a href="../articles/v09-fasterSetting.html">Tips for faster computation</a>
    </li>
    <li>
      <a href="../articles/v10-algorithm.html">ABESS algorithm: details</a>
    </li>
    <li>
      <a href="../articles/v11-power-of-abess.html">Power of abess</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/abess-team/abess/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
<form class="navbar-form navbar-right hidden-xs hidden-sm" role="search">
        <div class="form-group">
          <input type="search" class="form-control" name="search-input" id="search-input" placeholder="Search..." aria-label="Search for..." autocomplete="off">
</div>
      </form>
      
    </div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="v09-fasterSetting_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Tips for faster computation</h1>
                        <h4 data-toc-skip class="author">Jin Zhu</h4>
            
            <h4 data-toc-skip class="date">6/13/2021</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/abess-team/abess/tree/master/R-package/../vignettes/v09-fasterSetting.Rmd" class="external-link"><code>../vignettes/v09-fasterSetting.Rmd</code></a></small>
      <div class="hidden name"><code>v09-fasterSetting.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>The generic splicing technique certifiably guarantees the best subset can be selected in a polynomial time. In practice, the computational efficiency can be improved to handle large scale datasets. The tips for computational improvement include:</p>
<ul>
<li>exploit sparse structure of input matrix;</li>
<li>use golden-section to search best support size;<br>
</li>
<li>early-stop scheme;<br>
</li>
<li>sure independence screening;<br>
</li>
<li>warm-start initialization;<br>
</li>
<li>parallel computing when performing cross validation;<br>
</li>
<li>covariance update for <code>family = "gaussian"</code> or <code>family = "mgaussian"</code>;<br>
</li>
<li>approximate Newton iteration for <code>family = "binomial"</code>, <code>family = "poisson"</code>, <code>family = "cox"</code>.</li>
</ul>
<p>This vignette illustrate the first three tips. For the other tips, they have been efficiently implemented and set as the default in abess package.</p>
</div>
<div class="section level2">
<h2 id="sparse-matrix">Sparse matrix<a class="anchor" aria-label="anchor" href="#sparse-matrix"></a>
</h2>
<p>We sometimes meet with problems where the <span class="math inline">\(N \times p\)</span> input matrix <span class="math inline">\(X\)</span> is extremely sparse, i.e., many entries in <span class="math inline">\(X\)</span> have zero values. A notable example comes from document classification: aiming to assign classes to a document, making it easier to manage for publishers and news sites. The input variables for characterizing documents are generated from a so called ``bag-of-words’’ model. In this model, each variable is scored for the presence of each of the words in the entire dictionary under consideration. Since most words are absent, the input variables for each document is mostly zero, and so the entire matrix is mostly zero. Such sparse matrices can be efficiently stored in R with a <em>sparse column format</em> via the <a href="https://cran.r-project.org/web/packages/Matrix" class="external-link">Matrix</a> package. And the sparse matrix can be directly used by our <strong>abess</strong> package for boosting the computational efficient.</p>
<p>ABESS algorithm is ideally set up to exploit such sparsity. The <span class="math inline">\(O(N)\)</span> inner-product operations when computing forward sacrifice can exploit the sparsity, by summing over only the non-zero entries. For computing backward sacrifice, the sparsity also facilitate solving the convex optimization under a given support set. The following example demonstrates the efficiency gain from the sparse matrix. We first generate a input matrix whose 90% entries are 0.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">require</a></span><span class="op">(</span><span class="va"><a href="http://Matrix.R-forge.R-project.org/" class="external-link">Matrix</a></span><span class="op">)</span></code></pre></div>
<pre><code><span class="co">## Loading required package: Matrix</span></code></pre>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">num</span> <span class="op">&lt;-</span> <span class="fl">1000</span>
<span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">100</span>
<span class="va">sparse_ratio</span> <span class="op">&lt;-</span> <span class="fl">0.9</span>
<span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">num</span> <span class="op">*</span> <span class="va">p</span><span class="op">)</span>, nrow <span class="op">=</span> <span class="va">num</span><span class="op">)</span>
<span class="va">zero_entry</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html" class="external-link">rbinom</a></span><span class="op">(</span><span class="va">num</span> <span class="op">*</span> <span class="va">p</span>, size <span class="op">=</span> <span class="fl">1</span>, prob <span class="op">=</span> <span class="va">sparse_ratio</span><span class="op">)</span>, nrow <span class="op">=</span> <span class="va">num</span><span class="op">)</span>
<span class="va">x</span><span class="op">[</span><span class="va">zero_entry</span> <span class="op">==</span> <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">0</span>
<span class="va">y</span> <span class="op">&lt;-</span> <span class="va">x</span> <span class="op"><a href="https://rdrr.io/pkg/Matrix/man/matrix-products.html" class="external-link">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">5</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="va">p</span> <span class="op">-</span> <span class="fl">5</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>
<span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/Matrix.html" class="external-link">Matrix</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span><span class="op">)</span></code></pre></div>
<pre><code><span class="co">## 6 x 10 sparse Matrix of class "dgCMatrix"</span>
<span class="co">##                                                                 </span>
<span class="co">## [1,] .  .         .         -1.619439 .        .  .        . . .</span>
<span class="co">## [2,] . -0.6891435 0.7052694  .        .        . -1.311552 . . .</span>
<span class="co">## [3,] .  .         .          .        .        .  .        . . .</span>
<span class="co">## [4,] .  .         .          .        1.254155 .  .        . . .</span>
<span class="co">## [5,] .  .         .          .        .        .  .        . . .</span>
<span class="co">## [6,] .  .         .          .        .        .  .        . . .</span></code></pre>
<p>Then, we apply ABESS algorithm on Matrix <code>x</code> and record the runtime in <code>t1</code>:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/abess-team/abess" class="external-link">abess</a></span><span class="op">)</span>
<span class="va">t1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/system.time.html" class="external-link">system.time</a></span><span class="op">(</span><span class="va">abess_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/abess.html">abess</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>We compare the runtime when the input matrix is dense matrix:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">as.matrix</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">6</span><span class="op">]</span><span class="op">)</span></code></pre></div>
<pre><code><span class="co">##      [,1]       [,2]      [,3]      [,4]     [,5] [,6]</span>
<span class="co">## [1,]    0  0.0000000 0.0000000 -1.619439 0.000000    0</span>
<span class="co">## [2,]    0 -0.6891435 0.7052694  0.000000 0.000000    0</span>
<span class="co">## [3,]    0  0.0000000 0.0000000  0.000000 0.000000    0</span>
<span class="co">## [4,]    0  0.0000000 0.0000000  0.000000 1.254155    0</span>
<span class="co">## [5,]    0  0.0000000 0.0000000  0.000000 0.000000    0</span>
<span class="co">## [6,]    0  0.0000000 0.0000000  0.000000 0.000000    0</span></code></pre>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">t2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/system.time.html" class="external-link">system.time</a></span><span class="op">(</span><span class="va">abess_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/abess.html">abess</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">t1</span>, <span class="va">t2</span><span class="op">)</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span></code></pre></div>
<pre><code><span class="co">##    user.self sys.self elapsed</span>
<span class="co">## t1     1.306    0.044   1.392</span>
<span class="co">## t2     1.689    0.211   2.370</span></code></pre>
<p>From the comparison, we see that the time required by sparse matrix is visibly smaller, and thus, we suggest to assign a sparse matrix to <code>abess</code> when the input matrix have a lot of zero entries.</p>
</div>
<div class="section level2">
<h2 id="golden-section-searching">Golden-section searching<a class="anchor" aria-label="anchor" href="#golden-section-searching"></a>
</h2>
<p>The following is a typical ``model size v.s. BGIC’’ plot.<br><img src="sgsplicing.png" style="width:80.0%"></p>
<p>The <span class="math inline">\(x\)</span>-axis is model size, and the <span class="math inline">\(y\)</span>-axis is BGIC’s value recorded in group splicing algorithm for linear model. The entries of design matrix <span class="math inline">\(X\)</span> are <em>i.i.d.</em> sampled from <span class="math inline">\(\mathcal{N}(0, 1)\)</span>, and the matrix shape is <span class="math inline">\(100 \times 200\)</span>. The error term <span class="math inline">\(\varepsilon\)</span> are <em>i.i.d.</em> <span class="math inline">\(\mathcal{N}(0, \frac{1}{2})\)</span>. Take the two adjacent variables as one group, and set the true coefficients <span class="math inline">\(\beta=(1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 0, \ldots, 0)\)</span>. The orange vertical dash line indicates the true group subset size.</p>
<p>From this Figure, we see that the BGIC decreases from <span class="math inline">\(T=1\)</span> to <span class="math inline">\(T=5\)</span>, but it increases as <span class="math inline">\(T\)</span> larger than <span class="math inline">\(5\)</span>. In other words, the BGIC path of SGSplicing algorithm is a strictly unimodal function achieving minimum at the true group subset size <span class="math inline">\(T = 5\)</span>. Motivated by this observation, we suggest to recruit a heuristic search based on the golden-section search technique, an efficient method for finding the extremum of a unimodal function, to determine support size that minimizing BGIC. Compared with searching the optimal support size one by one from a candidate set with <span class="math inline">\(O(s_{\max})\)</span> complexity, golden-section reduce the time complexity to <span class="math inline">\(O(\ln{(s_{\max})})\)</span>, giving a significant computational improvement.</p>
<p>The code below exhibits how to employ the golden search technique with abess package:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">synthetic_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/generate.data.html">generate.data</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">500</span>, p <span class="op">=</span> <span class="fl">100</span>, 
                                beta <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">1.5</span>, <span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">2</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">95</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>
<span class="va">dat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">cbind.data.frame</a></span><span class="op">(</span><span class="st">"y"</span> <span class="op">=</span> <span class="va">synthetic_data</span><span class="op">[[</span><span class="st">"y"</span><span class="op">]</span><span class="op">]</span>, 
                        <span class="va">synthetic_data</span><span class="op">[[</span><span class="st">"x"</span><span class="op">]</span><span class="op">]</span><span class="op">)</span>
<span class="va">t1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/system.time.html" class="external-link">system.time</a></span><span class="op">(</span><span class="va">abess_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/abess.html">abess</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">dat</span>, tune.path <span class="op">=</span> <span class="st">"gsection"</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="fu"><a href="../reference/extract.abess.html">extract</a></span><span class="op">(</span><span class="va">abess_fit</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code><span class="co">## List of 7</span>
<span class="co">##  $ beta        :Formal class 'dgCMatrix' [package "Matrix"] with 6 slots</span>
<span class="co">##   .. ..@ i       : int [1:3] 0 1 4</span>
<span class="co">##   .. ..@ p       : int [1:2] 0 3</span>
<span class="co">##   .. ..@ Dim     : int [1:2] 100 1</span>
<span class="co">##   .. ..@ Dimnames:List of 2</span>
<span class="co">##   .. .. ..$ : chr [1:100] "x1" "x2" "x3" "x4" ...</span>
<span class="co">##   .. .. ..$ : chr "3"</span>
<span class="co">##   .. ..@ x       : num [1:3] 3.04 1.49 1.91</span>
<span class="co">##   .. ..@ factors : list()</span>
<span class="co">##  $ intercept   : num -0.0679</span>
<span class="co">##  $ support.size: int 3</span>
<span class="co">##  $ support.vars: chr [1:3] "x1" "x2" "x5"</span>
<span class="co">##  $ support.beta: num [1:3] 3.04 1.49 1.91</span>
<span class="co">##  $ dev         : num 1.49</span>
<span class="co">##  $ tune.value  : num 225</span></code></pre>
<p>The output of golden-section strategy suggests the optimal model size is accurately detected. Compare to the sequential searching, the golden section reduce the runtime because it skip some support sizes which are likely to be a non-optimal one:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">t2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/system.time.html" class="external-link">system.time</a></span><span class="op">(</span><span class="va">abess_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/abess.html">abess</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">dat</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">t1</span>, <span class="va">t2</span><span class="op">)</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span></code></pre></div>
<pre><code><span class="co">##    user.self sys.self elapsed</span>
<span class="co">## t1     0.044    0.003   0.075</span>
<span class="co">## t2     0.290    0.050   0.373</span></code></pre>
</div>
<div class="section level2">
<h2 id="early-stop">Early stop<a class="anchor" aria-label="anchor" href="#early-stop"></a>
</h2>
<p>In machine learning, early stopping is a helpful strategy not only avoid overfitting but also reducing training time. For the early-stopping implementation in abess, validation is used to detect when overfitting starts during performing adaptive best subset selection; training is then stopped even though the best subset under certain larger support size haven’t found. We give an example to demonstrate the helpfulness of early stopping in decreasing runtimes. (Do not finish, the early stopping do not available in cpp)</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">t1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/system.time.html" class="external-link">system.time</a></span><span class="op">(</span><span class="va">abess_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/abess.html">abess</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">dat</span>, early.stop <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span>
<span class="va">abess_fit</span></code></pre></div>
<pre><code><span class="co">## Call:</span>
<span class="co">## abess.formula(formula = y ~ ., data = dat, early.stop = TRUE)</span>
<span class="co">## </span>
<span class="co">##    support.size       dev       GIC</span>
<span class="co">## 1             0 17.280099 1424.7777</span>
<span class="co">## 2             1  7.791728 1034.9445</span>
<span class="co">## 3             2  3.963347  705.3708</span>
<span class="co">## 4             3  1.490205  224.6964</span>
<span class="co">## 5             4  1.467577  225.4590</span>
<span class="co">## 6             5  1.445940  226.4459</span>
<span class="co">## 7             6  1.432624  230.2331</span>
<span class="co">## 8             7  1.418358  233.6423</span>
<span class="co">## 9             8  1.404924  237.2973</span>
<span class="co">## 10            9  1.395503  242.3464</span>
<span class="co">## 11           10  1.386509  247.5264</span>
<span class="co">## 12           11  1.378860  253.1738</span>
<span class="co">## 13           12  1.369313  258.1130</span>
<span class="co">## 14           13  1.363112  264.2568</span>
<span class="co">## 15           14  1.354559  269.5226</span>
<span class="co">## 16           15  1.349460  276.0503</span>
<span class="co">## 17           16  1.342953  282.0464</span>
<span class="co">## 18           17  1.338077  288.6412</span>
<span class="co">## 19           18  1.335379  296.0453</span>
<span class="co">## 20           19  1.328946  302.0437</span>
<span class="co">## 21           20  1.322636  308.0772</span>
<span class="co">## 22           21  1.317662  314.6067</span>
<span class="co">## 23           22  1.315003  322.0100</span>
<span class="co">## 24           23  1.307019  327.3782</span>
<span class="co">## 25           24  1.305670  335.2750</span>
<span class="co">## 26           25  1.300286  341.6222</span>
<span class="co">## 27           26  1.296708  348.6575</span>
<span class="co">## 28           27  1.295496  356.6031</span>
<span class="co">## 29           28  1.292647  363.9154</span>
<span class="co">## 30           29  1.285055  369.3835</span>
<span class="co">## 31           30  1.280376  375.9729</span>
<span class="co">## 32           31  1.276758  382.9712</span>
<span class="co">## 33           32  1.273704  390.1868</span>
<span class="co">## 34           33  1.272467  398.1142</span>
<span class="co">## 35           34  1.268332  404.9001</span>
<span class="co">## 36           35  1.265358  412.1395</span>
<span class="co">## 37           36  1.261647  419.0841</span>
<span class="co">## 38           37  1.259729  426.7366</span>
<span class="co">## 39           38  1.264535  437.0538</span>
<span class="co">## 40           39  1.258498  443.0740</span>
<span class="co">## 41           40  1.255934  450.4676</span>
<span class="co">## 42           41  1.250641  456.7690</span>
<span class="co">## 43           42  1.249271  464.6346</span>
<span class="co">## 44           43  1.245465  471.5218</span>
<span class="co">## 45           44  1.244823  479.6775</span>
<span class="co">## 46           45  1.244839  488.0970</span>
<span class="co">## 47           46  1.241505  495.1693</span>
<span class="co">## 48           47  1.238645  502.4292</span>
<span class="co">## 49           48  1.235082  509.4021</span>
<span class="co">## 50           49  1.230958  516.1432</span>
<span class="co">## 51           50  1.226956  522.9279</span>
<span class="co">## 52           51  1.226910  531.3226</span>
<span class="co">## 53           52  1.223281  538.2545</span>
<span class="co">## 54           53  1.223886  546.9150</span>
<span class="co">## 55           54  1.220571  553.9721</span>
<span class="co">## 56           55  1.219292  561.8611</span>
<span class="co">## 57           56  1.219155  570.2183</span>
<span class="co">## 58           57  1.215722  577.2212</span>
<span class="co">## 59           58  1.213850  584.8642</span>
<span class="co">## 60           59  1.213127  592.9793</span></code></pre>
<p>We can see that ABESS algorithm stop when support size is 4. This is because the GIC value (can be considered as an assessment in validation set) do not increase when support size reach to 3, and thus, the program early terminate. This result is match to our simulation setting. Compare with the ABESS without early-stopping:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">t2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/system.time.html" class="external-link">system.time</a></span><span class="op">(</span><span class="va">abess_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/abess.html">abess</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">dat</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">t1</span>, <span class="va">t2</span><span class="op">)</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span></code></pre></div>
<pre><code><span class="co">##    user.self sys.self elapsed</span>
<span class="co">## t1     0.286    0.051   0.361</span>
<span class="co">## t2     0.279    0.039   0.320</span></code></pre>
<p>we can conclude that early-stopping brings fast computation and might maintain statistical guarantee.</p>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Jin Zhu, Liyuan Hu, Junhao Huang, Kangkang Jiang, Yanhang Zhang, Zezhi Wang, Borui Tang, Shiyun Lin, Junxian Zhu, Canhong Wen, Heping Zhang, Xueqin Wang.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.1.</p>
</div>

      </footer>
</div>

  
<script src="https://cdnjs.cloudflare.com/ajax/libs/docsearch.js/2.6.1/docsearch.min.js" integrity="sha256-GKvGqXDznoRYHCwKXGnuchvKSwmx9SRMrZOTh2g4Sb0=" crossorigin="anonymous"></script><script>
  docsearch({
    
    
    apiKey: 'd32715b0e35635336aba6377dd751e21',
    indexName: 'abess',
    inputSelector: 'input#search-input.form-control',
    transformData: function(hits) {
      return hits.map(function (hit) {
        hit.url = updateHitURL(hit);
        return hit;
      });
    }
  });
</script>
</body>
</html>
