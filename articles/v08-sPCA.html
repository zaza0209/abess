<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Principal component analysis • abess</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/cerulean/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Principal component analysis">
<meta property="og:description" content="abess">
<meta property="og:image" content="/logo.svg">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">abess</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.2.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/v01-abess-guide.html">Quick start for `abess`: Linear regression</a>
    </li>
    <li>
      <a href="../articles/v03-classification.html">Classification: Logistic Regression and Multinomial Extension</a>
    </li>
    <li>
      <a href="../articles/v04-poissonregression.html">Poisson Regression</a>
    </li>
    <li>
      <a href="../articles/v05-coxreg.html">Best Subset Selection for Censored Response</a>
    </li>
    <li>
      <a href="../articles/v06-MultiTaskLearning.html">Multi-Response Linear Regression</a>
    </li>
    <li>
      <a href="../articles/v07-advancedFeatures.html">Advanced Features</a>
    </li>
    <li>
      <a href="../articles/v08-sPCA.html">Principal component analysis</a>
    </li>
    <li>
      <a href="../articles/v09-fasterSetting.html">Tips for faster computation</a>
    </li>
    <li>
      <a href="../articles/v10-algorithm.html">ABESS algorithm: details</a>
    </li>
    <li>
      <a href="../articles/v11-power-of-abess.html">Power of abess</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/abess-team/abess/">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="v08-sPCA_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Principal component analysis</h1>
                        <h4 class="author">Junhao Huang, Jin Zhu</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/abess-team/abess/tree/master/R-package/../vignettes/v08-sPCA.Rmd"><code>../vignettes/v08-sPCA.Rmd</code></a></small>
      <div class="hidden name"><code>v08-sPCA.Rmd</code></div>

    </div>

    
    
<p>This vignettes introduces what is adaptive best subset selection principal component analysis (abessPCA) and use a real data example to show how to use it.</p>
<div id="introduction" class="section level2">
<h2 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h2>
<p>Principal component analysis (PCA) is an important method in the field of data science, which can reduce the dimension of data and simplify our model. It actually solve an optimization problem like:</p>
<p><span class="math display">\[
    \max_{v} v^{\top}\Sigma v,\qquad s.t.\quad v^{\top}v=1.
\]</span></p>
<p>where <span class="math inline">\(\Sigma = X^TX / (n-1)\)</span> and <span class="math inline">\(X\)</span> is the <strong>centered</strong> sample matrix. We also denote that <span class="math inline">\(X\)</span> is a <span class="math inline">\(n\times p\)</span> matrix, where each row is an observation and each column is a variables.</p>
<p>Then, before further analysis, we can project <span class="math inline">\(X\)</span> to <span class="math inline">\(v\)</span> (thus dimensional reduction), without losing too much information.</p>
<p>However, consider that: - The PC is a linear combination of all primary variables (<span class="math inline">\(Xv\)</span>), but sometimes we may tend to use less variables for clearer interpretation (and less computational complexity); - It has been proved that if <span class="math inline">\(p/n\)</span> does not converge to <span class="math inline">\(0\)</span>, the classical PCA is not consistent, but this would happen in some high-dimensional data analysis.</p>
<p>For example, in gene analysis, the dataset may contain plenty of genes (variables) and we would like to find a subset of them, which can explain most information. Compared with using all genes, this small subset may perform better on interpretation, without loss much information. Then we can focus on these variables in the further analysis.</p>
<p>When we trapped by these problems, a classical PCA may not be a best choice, since it use all variables. One of the alternatives is abessPCA, which is able to seek for principal component with a sparsity limitation:</p>
<p><span class="math display">\[
    \max_{v} v^{\top}\Sigma v,\qquad s.t.\quad v^{\top}v=1,\ ||v||_0\leq s.
\]</span></p>
<p>where <span class="math inline">\(s\)</span> is a non-negative integer, which indicates how many primary variables are used in principal component. With abessPCA, we can search for the best subset of variables to form principal component and it retains consistency even under <span class="math inline">\(p&gt;&gt;n\)</span>. And we make two remarks:</p>
<ul>
<li>Clearly, if <span class="math inline">\(s\)</span> is equal or larger than the number of primary variables, this sparsity limitation is actually useless, so the problem is equivalent to a classical PCA.</li>
<li>With less variables, the PC must have lower explained variance. However, this decrease is slight if we choose a good <span class="math inline">\(s\)</span> and at this price, we can interpret the PC much better. It is worthy.</li>
</ul>
<p>In the next section, we will show how to form abessPCA in our frame.</p>
</div>
<div id="abesspca-real-data-example" class="section level2">
<h2 class="hasAnchor">
<a href="#abesspca-real-data-example" class="anchor"></a>abessPCA: real data example</h2>
<div id="communities-and-crime-dataset" class="section level3">
<h3 class="hasAnchor">
<a href="#communities-and-crime-dataset" class="anchor"></a>Communities-and-crime dataset</h3>
<p>Here we will use real data analysis to show how to form abessPCA. The data we use is from <a href="https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime">UCI: Communities and Crime Data Set</a>.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.csv</a></span><span class="op">(</span><span class="st">'./communities.data'</span>, header <span class="op">=</span> <span class="cn">FALSE</span>, na.strings <span class="op">=</span> <span class="st">'?'</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 1994  128</code></pre>
<p>The dataset contain 128 variables but a part of them have mising values or categorical variables. We simply drop these variables, and retain 99 predictive variables as our data example.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">X</span> <span class="op">&lt;-</span> <span class="va">X</span><span class="op">[</span>, <span class="fl">6</span><span class="op">:</span><span class="fl">127</span><span class="op">]</span>
<span class="va">na_col</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">X</span>, <span class="fl">2</span>, <span class="va">anyNA</span><span class="op">)</span>
<span class="va">X</span> <span class="op">&lt;-</span> <span class="va">X</span><span class="op">[</span>, <span class="op">!</span><span class="va">na_col</span><span class="op">]</span>
<span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 1994   99</code></pre>
</div>
<div id="adaptive-best-subset-selection-for-pca" class="section level3">
<h3 class="hasAnchor">
<a href="#adaptive-best-subset-selection-for-pca" class="anchor"></a>Adaptive best subset selection for PCA</h3>
<p>Next, we turn to fit abessPCA. For fitting the model, we can give either predictor matrix <span class="math inline">\(X\)</span>:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/abess-team/abess">abess</a></span><span class="op">)</span>
<span class="va">best_pca</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/abesspca.html">abesspca</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">best_pca</span><span class="op">)</span></code></pre></div>
<pre><code>## List of 8
##  $ nvars       : int 99
##  $ support.size: num [1:99] 1 2 3 4 5 6 7 8 9 10 ...
##  $ sparse.type : chr "fpc"
##  $ coef        :Formal class 'dgCMatrix' [package "Matrix"] with 6 slots
##   .. ..@ i       : int [1:4950] 11 11 76 11 76 83 11 81 82 83 ...
##   .. ..@ p       : int [1:100] 0 1 3 6 10 15 21 28 36 45 ...
##   .. ..@ Dim     : int [1:2] 99 99
##   .. ..@ Dimnames:List of 2
##   .. .. ..$ : chr [1:99] "V6" "V7" "V8" "V9" ...
##   .. .. ..$ : chr [1:99] "1" "2" "3" "4" ...
##   .. ..@ x       : num [1:4950] 1 0.96 -0.28 -0.899 0.31 ...
##   .. ..@ factors : list()
##  $ ev          : num [1:99] 0.198 0.211 0.228 0.247 0.27 ...
##  $ pev         : num [1:99] 0.186 0.198 0.214 0.232 0.254 ...
##  $ var.all     : num 1.07
##  $ call        : language abesspca(x = X)
##  - attr(*, "class")= chr "abesspca"</code></pre>
<p>or Gram-type matrix (like covariance matrix, correlation matrix and robust covariance matrix):</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">best_pca</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/abesspca.html">abesspca</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cov</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>, type <span class="op">=</span> <span class="st">"gram"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">best_pca</span><span class="op">)</span></code></pre></div>
<pre><code>## List of 8
##  $ nvars       : int 99
##  $ support.size: num [1:99] 1 2 3 4 5 6 7 8 9 10 ...
##  $ sparse.type : chr "fpc"
##  $ coef        :Formal class 'dgCMatrix' [package "Matrix"] with 6 slots
##   .. ..@ i       : int [1:4950] 11 11 76 11 76 83 11 81 82 83 ...
##   .. ..@ p       : int [1:100] 0 1 3 6 10 15 21 28 36 45 ...
##   .. ..@ Dim     : int [1:2] 99 99
##   .. ..@ Dimnames:List of 2
##   .. .. ..$ : chr [1:99] "V6" "V7" "V8" "V9" ...
##   .. .. ..$ : chr [1:99] "1" "2" "3" "4" ...
##   .. ..@ x       : num [1:4950] 1 0.96 -0.28 -0.899 0.31 ...
##   .. ..@ factors : list()
##  $ ev          : num [1:99] 0.198 0.211 0.228 0.247 0.27 ...
##  $ pev         : num [1:99] 0.186 0.198 0.214 0.232 0.254 ...
##  $ var.all     : num 1.07
##  $ call        : language abesspca(x = cov(X), type = "gram")
##  - attr(*, "class")= chr "abesspca"</code></pre>
</div>
<div id="interpreting-result" class="section level3">
<h3 class="hasAnchor">
<a href="#interpreting-result" class="anchor"></a>Interpreting result</h3>
<p>After fitting abessPCA, we study the percentage of explained variance as <span class="math inline">\(s\)</span> increases:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">best_pca</span><span class="op">[[</span><span class="st">"support.size"</span><span class="op">]</span><span class="op">]</span>, <span class="va">best_pca</span><span class="op">[[</span><span class="st">"pev"</span><span class="op">]</span><span class="op">]</span>, type <span class="op">=</span> <span class="st">"l"</span><span class="op">)</span></code></pre></div>
<p><img src="v08-sPCA_files/figure-html/unnamed-chunk-5-1.png" width="700"></p>
<p>It is clear that the higher sparsity is, the more variance it can explain. Interestingly, we can seek for a smaller sparsity which can explain most of the variance. For instance, when 40 variables are selected, the percentage of explained variance from abessPCA exceeds 80%.<br>
This result shows that using less than half of all 99 variables can be close to perfect. We can use <code>coef</code> function to investigate which variables are selected when the explained variance are large. For example, if we choose sparsity 40, the used variables are:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">best_pca</span>, support.size <span class="op">=</span> <span class="fl">40</span><span class="op">)</span></code></pre></div>
<pre><code>## 99 x 1 sparse Matrix of class "dgCMatrix"
##               40
## V6    .         
## V7    .         
## V8    .         
## V9    .         
## V10   .         
## V11   .         
## V12   .         
## V13   .         
## V14   .         
## V15   .         
## V16   .         
## V17  -0.20426441
## V18  -0.20998764
## V19   .         
## V20   .         
## V21  -0.15497535
## V22   .         
## V23   0.17606343
## V24   .         
## V25  -0.20009707
## V26  -0.18203272
## V27  -0.16485786
## V28  -0.12120442
## V29   .         
## V30   .         
## V32  -0.12473579
## V33   .         
## V34   0.19891632
## V35   0.15660866
## V36   0.16775785
## V37  -0.16295849
## V38   0.15683970
## V39  -0.11967714
## V40   .         
## V41   .         
## V42   0.15205308
## V43  -0.15186760
## V44   0.10898654
## V45   .         
## V46   .         
## V47   .         
## V48   .         
## V49  -0.16125659
## V50  -0.16569592
## V51  -0.18161686
## V52  -0.12941861
## V53   .         
## V54   .         
## V55   .         
## V56   .         
## V57   .         
## V58   .         
## V59   .         
## V60   .         
## V61   .         
## V62   .         
## V63   .         
## V64  -0.04321994
## V65  -0.04009683
## V66   .         
## V67   .         
## V68   .         
## V69   .         
## V70   .         
## V71   .         
## V72   .         
## V73  -0.11501248
## V74   .         
## V75   0.09077053
## V76  -0.10110737
## V77   .         
## V78   .         
## V79  -0.09781943
## V80   0.10484204
## V81   .         
## V82   .         
## V83   0.21424871
## V84   .         
## V85  -0.19681645
## V86  -0.20101165
## V87  -0.20156244
## V88  -0.19934525
## V89  -0.19581502
## V90  -0.23212384
## V91  -0.19894919
## V92   .         
## V93  -0.08278438
## V94   .         
## V95   .         
## V96   .         
## V97   .         
## V98   0.07237337
## V99   .         
## V100  .         
## V101  .         
## V119  .         
## V120  .         
## V121 -0.06753651
## V126  .</code></pre>
<p>where each row of loading matrix corresponds to a variable.</p>
</div>
</div>
<div id="extension" class="section level2">
<h2 class="hasAnchor">
<a href="#extension" class="anchor"></a>Extension</h2>
<div id="group-variable" class="section level3">
<h3 class="hasAnchor">
<a href="#group-variable" class="anchor"></a>Group variable</h3>
<p>In some cases, some variables may need to consider together, that is, they should be “used” or “unused” for PC at the same time, which we call “group information”. The optimization problem becomes:</p>
<p><span class="math display">\[
    \max_{v} v^{\top}\Sigma v,\qquad s.t.\quad v^{\top}v=1,\ \sum_{g=1}^G I(||v_g||\neq 0)\leq s.
\]</span></p>
<p>where we suppose there are <span class="math inline">\(G\)</span> groups, and the <span class="math inline">\(g\)</span>-th one correspond to <span class="math inline">\(v_g\)</span>, <span class="math inline">\(v = [v_1^T,v_2^T,\cdots,v_G^T]^T\)</span>. Then we are interested to find <span class="math inline">\(s\)</span> (or less) important groups.</p>
<blockquote>
<p>Group problem is extraordinary important in real data analysis. Still take gene analysis as an example, several sites would be related to one pathway, and it is meaningless to consider each of them alone.</p>
</blockquote>
<p>abessPCA can also deal with group information. Here we make sure that variables in the same group address close to each other (if not, the data should be sorted first).</p>
<p>Suppose that the data above have group information like:</p>
<ul>
<li>Group 1: {the 1st, 2nd, …, 6th variable};</li>
<li>Group 2: {the 7th, 8th, …, 12th variable};</li>
<li>…</li>
<li>Group 16: {the 91st, 92nd, …, 96th variable};</li>
<li>Group 17: {the 97th, 98th, 99th variables}.</li>
</ul>
<p>Denote different groups as different number:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">g_info</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">16</span>, each <span class="op">=</span> <span class="fl">6</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">17</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>And fit a group SPCA model with additional argument <code>group.index = g_info</code>:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">best_pca</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/abesspca.html">abesspca</a></span><span class="op">(</span><span class="va">X</span>, group.index <span class="op">=</span> <span class="va">g_info</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">best_pca</span><span class="op">)</span></code></pre></div>
<pre><code>## List of 8
##  $ nvars       : int 99
##  $ support.size: num [1:17] 1 2 3 4 5 6 7 8 9 10 ...
##  $ sparse.type : chr "fpc"
##  $ coef        :Formal class 'dgCMatrix' [package "Matrix"] with 6 slots
##   .. ..@ i       : int [1:912] 42 43 44 45 46 47 18 19 20 21 ...
##   .. ..@ p       : int [1:18] 0 6 18 36 60 90 126 168 216 270 ...
##   .. ..@ Dim     : int [1:2] 99 17
##   .. ..@ Dimnames:List of 2
##   .. .. ..$ : chr [1:99] "V6" "V7" "V8" "V9" ...
##   .. .. ..$ : chr [1:17] "1" "2" "3" "4" ...
##   .. ..@ x       : num [1:912] -0.503 -0.512 -0.5279 -0.4516 0.0472 ...
##   .. ..@ factors : list()
##  $ ev          : num [1:17] 0.158 0.388 0.471 0.562 0.641 ...
##  $ pev         : num [1:17] 0.148 0.364 0.442 0.527 0.602 ...
##  $ var.all     : num 1.07
##  $ call        : language abesspca(x = X, group.index = g_info)
##  - attr(*, "class")= chr "abesspca"</code></pre>
</div>
<div id="multiple-principal-components" class="section level3">
<h3 class="hasAnchor">
<a href="#multiple-principal-components" class="anchor"></a>Multiple principal components</h3>
<p>In some cases, we may seek for more than one principal components under sparsity. Actually, we can iteratively solve the largest principal component and then mapping the covariance matrix to its orthogonal space:</p>
<p><span class="math display">\[
\Sigma' = (1-vv^{\top})\Sigma(1-vv^{\top})
\]</span></p>
<p>where <span class="math inline">\(\Sigma\)</span> is the currect covariance matrix and <span class="math inline">\(v\)</span> is its (sparse) loading vector. We map it into <span class="math inline">\(\Sigma'\)</span>, which indicates the orthogonal space of <span class="math inline">\(v\)</span>, and then solve the sparse principal component for <span class="math inline">\(\Sigma'\)</span> again. By this iteration process, we can acquire multiple principal components and they are sorted from the largest to the smallest. In our program, there is an additional argument <code>sparse.type</code> to support this feature. By setting <code>sparse.type = "kpc"</code>, then best subset selection performs on the first <span class="math inline">\(K\)</span> principal components where <span class="math inline">\(K\)</span> is decided by argument <code>support.size</code>.</p>
<p>Suppose we are interested in the first two principal components, and the support size is 50 in the first loading vector and is 40 in the second loading vector. In other words, we consecutively solve two problem: <span class="math display">\[
    v_1 \leftarrow \arg\max_{v} v^{\top}\Sigma v,\qquad s.t.\quad v^{\top}v=1,\ ||v||_0\leq 10,
\]</span> <span class="math display">\[
  v_2 \leftarrow \arg\max_{v} v^{\top} \Sigma^{\prime} v,\qquad s.t.\quad v^{\top}v=1,\ ||v||_0\leq 5,
\]</span> where <span class="math inline">\(\Sigma^{\prime} = (1-v_1 v_1^\top)\Sigma(1-v_1 v_1^\top)\)</span>. The <span class="math inline">\((v_1, v_2)\)</span> forms a sparse loading matrix.</p>
<p>The code for solving the two problem is:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">best_kpca</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/abesspca.html">abesspca</a></span><span class="op">(</span><span class="va">X</span>, support.size <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">50</span>, <span class="fl">40</span><span class="op">)</span>, sparse.type <span class="op">=</span> <span class="st">"kpc"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">best_kpca</span><span class="op">)</span></code></pre></div>
<pre><code>## List of 8
##  $ nvars       : int 99
##  $ support.size: num [1:2] 50 40
##  $ sparse.type : chr "kpc"
##  $ coef        :Formal class 'dgCMatrix' [package "Matrix"] with 6 slots
##   .. ..@ i       : int [1:90] 4 5 6 7 8 9 11 12 13 15 ...
##   .. ..@ p       : int [1:3] 0 50 90
##   .. ..@ Dim     : int [1:2] 99 2
##   .. ..@ Dimnames:List of 2
##   .. .. ..$ : chr [1:99] "V6" "V7" "V8" "V9" ...
##   .. .. ..$ : chr [1:2] "50" "40"
##   .. ..@ x       : num [1:90] 0.0988 -0.0443 -0.0551 -0.0441 -0.0482 ...
##   .. ..@ factors : list()
##  $ ev          : num [1:2] 0.946 1.625
##  $ pev         : num [1:2] 0.238 0.41
##  $ var.all     : num 3.97
##  $ call        : language abesspca(x = X, sparse.type = "kpc", support.size = c(50, 40))
##  - attr(*, "class")= chr "abesspca"</code></pre>
<p>The result <code>best_kpca[["pev"]]</code> shows that two principal components raised from two loading matrix could explain 40% variance of all variables (i.e., <span class="math inline">\(\text{trace}(\Sigma)\)</span>).</p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Jin Zhu, Kangkang Jiang, Yanhang Zhang, Liyuan Hu, Junhao Huang, Junxian Zhu, Canhong Wen, Heping Zhang, Xueqin Wang.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
