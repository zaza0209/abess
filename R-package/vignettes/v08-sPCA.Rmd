---
title: "Principal component analysis"
author: "Junhao Huang, Jin Zhu"
output:
  html_document: 
    toc: yes
    keep_md: yes
    self_contained: no
  pdf_document:
    fig_caption: yes
    toc: yes
    toc_depth: 3
  word_document: 
    toc: yes
    keep_md: yes
---

## Introduction
Principal component analysis (PCA) is an important method in the field of data science, which can reduce the dimension of data and simplify our model. It actually solve an optimization problem like:

$$
    \max_{v} v^{\top}\Sigma v,\qquad s.t.\quad v^{\top}v=1.
$$

where $\Sigma = X^TX / (n-1)$ and $X$ is the **centered** sample matrix. We also denote that $X$ is a $n\times p$ matrix, where each row is an observation and each column is a variable.

Then, before further analysis, we can project $X$ to $v$ (thus dimensional reduction), without losing too much information.

However, consider that: 

- The PC is a linear combination of all primary variables ($Xv$), but sometimes we may tend to use less variables for clearer interpretation (and less computational complexity);

- It has been proved that if $p/n$ does not converge to $0$, the classical PCA is not consistent, but this would happen in some high-dimensional data analysis.

For example, in gene analysis, the dataset may contain plenty of genes (variables) and we would like to find a subset of them, which can explain most information. Compared with using all genes, this small subset may perform better on interpretation, without loss much information. Then we can focus on these variables in the further analysis.

When we are trapped by these problems, a classical PCA may not be a best choice, since it use all variables. One of the alternatives is abessPCA, which is able to seek for principal component with a sparsity limitation:

$$
    \max_{v} v^{\top}\Sigma v,\qquad s.t.\quad v^{\top}v=1,\ ||v||_0\leq s.
$$

where $s$ is a non-negative integer, which indicates how many primary variables are used in principal component. With abessPCA, we can search for the best subset of variables to form principal component and it retains consistency even under $p>>n$.
And we make two remarks:        

- Clearly, if $s$ is equal or larger than the number of primary variables, this sparsity limitation is actually useless, so the problem is equivalent to a classical PCA.
- With less variables, the PC must have lower explained variance. However, this decrease is slight if we choose a good $s$ and at this price, we can interpret the PC much better. It is worthy. 

In the next section, we will show how to form abessPCA in our frame.

## abessPCA: real data example    
### Communities-and-crime dataset
Here we will use real data analysis to show how to form abessPCA. The data we use is from [UCI:
Communities and Crime Data Set](https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime). 

```{r}
X <- read.csv('./communities.data', header = FALSE, na.strings = '?')
dim(X)
```

The dataset contain 128 variables but a part of them have missing values or categorical variables. 
We simply drop these variables, and 
retain 99 predictive variables as our data example.

```{r}
X <- X[, 6:127]
na_col <- apply(X, 2, anyNA)
X <- X[, !na_col]
dim(X)
```

### Adaptive best subset selection for PCA        
Next, we turn to fit abessPCA. 
For fitting the model, we can give either predictor matrix $X$:

```{r}
library(abess)
best_pca <- abesspca(X)
str(best_pca)
```

or Gram-type matrix (like covariance matrix, correlation matrix and robust covariance matrix):        
```{r}
best_pca <- abesspca(cov(X), type = "gram")
str(best_pca)
```

### Interpreting result

After fitting abessPCA, we study the percentage of explained variance as $s$ increases:          
```{r}
plot(best_pca[["support.size"]], best_pca[["pev"]], type = "l")
```
    
It is clear that the higher sparsity is, the more variance it can explain. 
Interestingly, we can seek for a smaller sparsity which can explain most of the variance. 
For instance, when 40 variables are selected, 
the percentage of explained variance from abessPCA exceeds 80%.     
This result shows that using less than half of all 99 variables can be close to perfect. 
We can use `coef` function 
to investigate which variables are selected when the explained variance are large. 
For example, if we choose sparsity 40, the used variables are:
```{r}
coef(best_pca, support.size = 40)
```
where each row of loading matrix corresponds to a variable. 

## Extension
### Group variable        
In some cases, some variables may need to consider together, that is, they should be "used" or "unused" for PC at the same time, which we call "group information". The optimization problem becomes:

$$
    \max_{v} v^{\top}\Sigma v,\qquad s.t.\quad v^{\top}v=1,\ \sum_{g=1}^G I(||v_g||\neq 0)\leq s.
$$

where we suppose there are $G$ groups, and the $g$-th one correspond to $v_g$, $v = [v_1^T,v_2^T,\cdots,v_G^T]^T$. Then we are interested to find $s$ (or less) important groups.

Group problem is extraordinary important in real data analysis. Still take gene analysis as an example, several sites would be related to one pathway, and it is meaningless to consider each of them alone. 

abessPCA can also deal with group information. Here we make sure that variables in the same group address close to each other (if not, the data should be sorted first).

Suppose that the data above have group information like:

- Group 1: {the 1st, 2nd, ..., 6th variable};
- Group 2: {the 7th, 8th, ..., 12th variable};
- ...
- Group 16: {the 91st, 92nd, ..., 96th variable};
- Group 17: {the 97th, 98th, 99th variables}.

Denote different groups as different number:  
```{r}
g_info <- c(rep(1:16, each = 6), rep(17, 3))
```

And fit a group SPCA model with additional argument `group.index = g_info`:
```{r}
best_pca <- abesspca(X, group.index = g_info)
str(best_pca)
```

### Multiple principal components

In some cases, we may seek for more than one principal components under sparsity. 
Actually, we can iteratively solve the largest principal component and then mapping the covariance matrix to its orthogonal space:

$$
\Sigma' = (1-vv^{\top})\Sigma(1-vv^{\top})
$$

where $\Sigma$ is the covariance matrix and $v$ is its (sparse) loading vector. 
We map it into $\Sigma'$, which indicates the orthogonal space of $v$, 
and then solve the sparse principal component for $\Sigma'$ again. 
By this iteration process, we can acquire multiple principal components and they are sorted from the largest to the smallest.
In our program, there is an additional argument `sparse.type` to support this feature. 
By setting `sparse.type = "kpc"`, then best subset selection performs on the first $K$ principal components 
where $K$ is decided by argument `support.size`.    

Suppose we are interested in the first two principal components,
and the support size is 50 in the first loading vector and is 40 in the second loading vector. 
In other words, we consecutively solve two problem:
$$
    v_1 \leftarrow \arg\max_{v} v^{\top}\Sigma v,\qquad s.t.\quad v^{\top}v=1,\ ||v||_0\leq 10,
$$
$$
  v_2 \leftarrow \arg\max_{v} v^{\top} \Sigma^{\prime} v,\qquad s.t.\quad v^{\top}v=1,\ ||v||_0\leq 5,
$$
where $\Sigma^{\prime} = (1-v_1 v_1^\top)\Sigma(1-v_1 v_1^\top)$. 
The $(v_1, v_2)$ forms a sparse loading matrix.

The code for solving the two problem is:
```{r}
best_kpca <- abesspca(X, support.size = c(50, 40), sparse.type = "kpc")
str(best_kpca)
```

The result `best_kpca[["pev"]]` shows that two principal components raised from two loading matrix 
could explain 40\% variance of all variables (i.e., $\text{trace}(\Sigma)$). 
