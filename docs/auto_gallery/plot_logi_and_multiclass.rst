
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_gallery\plot_logi_and_multiclass.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_gallery_plot_logi_and_multiclass.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_gallery_plot_logi_and_multiclass.py:


Logistic Regression and Multinomial Extension
===============================================
# We would like to use an example to show how the best subset selection for logistic regression work in our program.

.. GENERATED FROM PYTHON SOURCE LINES 8-14

Real Data Example
------------------------
Titanic Dataset
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Consider the Titanic dataset obtained from the Kaggle competition: https://www.kaggle.com/c/titanic/data. 
The dataset consists of data about 889 passengers, and the goal of the competition is to predict the survival (yes/no) based on features including the class of service, the sex, the age etc. 

.. GENERATED FROM PYTHON SOURCE LINES 14-22

.. code-block:: default



    import numpy as np 
    import pandas as pd

    dt = pd.read_csv("train.csv")
    print(dt.head(5))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

       PassengerId  Survived  Pclass                                               Name     Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked
    0            1         0       3                            Braund, Mr. Owen Harris    male  22.0      1      0         A/5 21171   7.2500   NaN        S
    1            2         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1      0          PC 17599  71.2833   C85        C
    2            3         1       3                             Heikkinen, Miss. Laina  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S
    3            4         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1      0            113803  53.1000  C123        S
    4            5         0       3                           Allen, Mr. William Henry    male  35.0      0      0            373450   8.0500   NaN        S




.. GENERATED FROM PYTHON SOURCE LINES 23-27

We only focus on some numeric or classification variables:

- predictor variables: :math:`Pclass,\ Sex,\ Age,\ SibSp,\ Parch,\ Fare,\ Embarked`;
- response variable is :math:`Survived`.

.. GENERATED FROM PYTHON SOURCE LINES 27-34

.. code-block:: default




    dt = dt.iloc[:, [1,2,4,5,6,7,9,11]] # variables interested
    dt['Pclass'] = dt['Pclass'].astype(str)
    print(dt.head(5))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

       Survived Pclass     Sex   Age  SibSp  Parch     Fare Embarked
    0         0      3    male  22.0      1      0   7.2500        S
    1         1      1  female  38.0      1      0  71.2833        C
    2         1      3  female  26.0      0      0   7.9250        S
    3         1      1  female  35.0      1      0  53.1000        S
    4         0      3    male  35.0      0      0   8.0500        S




.. GENERATED FROM PYTHON SOURCE LINES 35-36

However, some rows contain missing value (NaN) and we need to drop them.

.. GENERATED FROM PYTHON SOURCE LINES 36-42

.. code-block:: default




    dt = dt.dropna()
    print('sample size: ', dt.shape)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    sample size:  (712, 8)




.. GENERATED FROM PYTHON SOURCE LINES 43-44

Then use dummy variables to replace classification variables:

.. GENERATED FROM PYTHON SOURCE LINES 44-50

.. code-block:: default




    dt1 = pd.get_dummies(dt)
    print(dt1.head(5))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

       Survived   Age  SibSp  Parch     Fare  Pclass_1  Pclass_2  Pclass_3  Sex_female  Sex_male  Embarked_C  Embarked_Q  Embarked_S
    0         0  22.0      1      0   7.2500         0         0         1           0         1           0           0           1
    1         1  38.0      1      0  71.2833         1         0         0           1         0           1           0           0
    2         1  26.0      0      0   7.9250         0         0         1           1         0           0           0           1
    3         1  35.0      1      0  53.1000         1         0         0           1         0           0           0           1
    4         0  35.0      0      0   8.0500         0         0         1           0         1           0           0           1




.. GENERATED FROM PYTHON SOURCE LINES 51-52

Now we split `dt1` into training set and testing set:

.. GENERATED FROM PYTHON SOURCE LINES 52-62

.. code-block:: default



    from sklearn.model_selection import train_test_split
    X = np.array(dt1.drop('Survived', axis = 1))
    Y = np.array(dt1.Survived)

    train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size = 0.33, random_state = 0)
    print('train size: ', train_x.shape[0])
    print('test size:', test_x.shape[0])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    train size:  477
    test size: 235




.. GENERATED FROM PYTHON SOURCE LINES 63-76

Here `train_x` contains:

- V0: dummy variable, 1st ticket class (1-yes, 0-no)
- V1: dummy variable, 2nd ticket class (1-yes, 0-no)
- V2: dummy variable, sex (1-male, 0-female)
- V3: Age
- V4: # of siblings / spouses aboard the Titanic
- V5: # of parents / children aboard the Titanic
- V6: Passenger fare
- V7: dummy variable, Cherbourg for embarkation (1-yes, 0-no)
- V8: dummy variable, Queenstown for embarkation (1-yes, 0-no)

And `train_y` indicates whether the passenger survived (1-yes, 0-no).

.. GENERATED FROM PYTHON SOURCE LINES 76-80

.. code-block:: default


    print('train_x:\n', train_x[0:5, :])
    print('train_y:\n', train_y[0:5])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    train_x:
     [[54.      1.      0.     59.4     1.      0.      0.      1.      0.
       1.      0.      0.    ]
     [30.      0.      0.      8.6625  0.      0.      1.      1.      0.
       0.      0.      1.    ]
     [47.      0.      0.     38.5     1.      0.      0.      0.      1.
       0.      0.      1.    ]
     [28.      2.      0.      7.925   0.      0.      1.      0.      1.
       0.      0.      1.    ]
     [29.      1.      0.     26.      0.      1.      0.      1.      0.
       0.      0.      1.    ]]
    train_y:
     [1 0 0 0 1]




.. GENERATED FROM PYTHON SOURCE LINES 81-84

Model Fitting
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
The `LogisticRegression()` function in the `abess.linear` allows you to perform best subset selection in a highly efficient way. For example, in the Titanic sample, if you want to look for a best subset with no more than 5 variables on the logistic model, you can call:

.. GENERATED FROM PYTHON SOURCE LINES 84-92

.. code-block:: default



    from abess.linear import LogisticRegression

    s = 5   # max target sparsity
    model = LogisticRegression(support_size = range(0, s + 1))
    model.fit(train_x, train_y)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    LogisticRegression(always_select=[], support_size=range(0, 6))



.. GENERATED FROM PYTHON SOURCE LINES 93-94

Now the `model.coef_` contains the coefficients of logistic model with no more than 5 variables. That is, those variables with a coefficient 0 is unused in the model: 

.. GENERATED FROM PYTHON SOURCE LINES 94-99

.. code-block:: default




    print(model.coef_)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [-0.05410776 -0.53642966  0.          0.          1.74091231  0.
     -1.26223831  0.         -2.7096497   0.          0.          0.        ]




.. GENERATED FROM PYTHON SOURCE LINES 100-103

By default, the `LogisticRegression` function set the `support_size = range(0, min(p,n/log(n)p)` and the best support size is determined by theExtended Bayesian Information Criteria (EBIC). You can change the tunging criterion by specifying the argument `ic_type`. The available tuning criterion now are `gic`, `aic`, `bic`, `ebic`. 

For a quicker solution, you can change the tuning strategy to a golden section path which trys to find the elbow point of the tuning criterion over the hyperparameter space. Here we give an example.

.. GENERATED FROM PYTHON SOURCE LINES 103-109

.. code-block:: default



    model_gs = LogisticRegression(path_type = "gs", s_min = 0, s_max = s)
    model_gs.fit(train_x, train_y)
    print(model_gs.coef_)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [-0.05410776 -0.53642966  0.          0.          1.74091231  0.
     -1.26223831  2.7096497   0.          0.          0.          0.        ]




.. GENERATED FROM PYTHON SOURCE LINES 110-111

where `s_min` and `s_max` bound the support size and this model give the same answer as before.

.. GENERATED FROM PYTHON SOURCE LINES 111-122

.. code-block:: default


    # More on the Results
    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    # After fitting with `model.fit()`, we can further do more exploring work to interpret it. 
    # As we show above, `model.coef_` contains the sparse coefficients of variables and those non-zero values indicates "important" varibles chosen in the model.


    print('Intercept: ', model.intercept_)
    print('coefficients: \n', model.coef_)
    print('Used variables\' index:', np.nonzero(model.coef_ != 0)[0])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Intercept:  [3.28394745]
    coefficients: 
     [-0.05410776 -0.53642966  0.          0.          1.74091231  0.
     -1.26223831  0.         -2.7096497   0.          0.          0.        ]
    Used variables' index: [0 1 4 6 8]




.. GENERATED FROM PYTHON SOURCE LINES 123-124

The training loss and the score under information criterion:

.. GENERATED FROM PYTHON SOURCE LINES 124-129

.. code-block:: default



    print('Training Loss: ', model.train_loss_)
    print('IC: ', model.ic_)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Training Loss:  [204.35270048]
    IC:  [464.39204991]




.. GENERATED FROM PYTHON SOURCE LINES 130-131

Prediction is allowed for the estimated model. Just call `model.predict()` function like: 

.. GENERATED FROM PYTHON SOURCE LINES 131-135

.. code-block:: default


    fitted_y = model.predict(test_x)
    print(fitted_y)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0.
     1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.
     0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1.
     1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0.
     1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
     0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1.
     1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.
     0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1.
     1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0.
     1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]




.. GENERATED FROM PYTHON SOURCE LINES 136-137

Besides, you can also call for the survival probability of each observation by `model.predict_proba()`. Actually, those who with a probability greater than 0.5 is classified to "1" (survived).

.. GENERATED FROM PYTHON SOURCE LINES 137-143

.. code-block:: default




    fitted_p = model.predict_proba(test_x)
    print(fitted_p)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [0.49256613 0.25942968 0.84928463 0.20204183 0.03801548 0.04022349
     0.72351443 0.23115622 0.23115622 0.66834673 0.96775535 0.64905946
     0.98461921 0.15238867 0.25004079 0.57640212 0.26995968 0.71264582
     0.37791835 0.1771314  0.25773297 0.75392142 0.87974411 0.40251569
     0.56441882 0.34057869 0.22005156 0.067159   0.57880531 0.33647767
     0.15655122 0.02682661 0.14553043 0.69663788 0.89078445 0.87925152
     0.91926004 0.59081387 0.42997279 0.45653474 0.38846964 0.09020182
     0.05742461 0.07773719 0.0994852  0.11006334 0.9819574  0.14219863
     0.1096089  0.96940171 0.71351188 0.69663788 0.63663757 0.25942968
     0.54978583 0.53309793 0.07032472 0.0706292  0.86889888 0.37901167
     0.43876674 0.03084541 0.14553043 0.19993615 0.29180956 0.11828599
     0.94586145 0.30610513 0.98763221 0.80911714 0.25942968 0.93051703
     0.9097025  0.51285362 0.04924417 0.53765354 0.48242039 0.26040948
     0.09474175 0.3384564  0.55107315 0.88025271 0.09058398 0.81733446
     0.86836852 0.09474175 0.04461544 0.28075505 0.78890012 0.13893026
     0.02434171 0.04697945 0.70146853 0.91404969 0.66232291 0.0994852
     0.93719603 0.8422183  0.1096089  0.15469685 0.15238867 0.85879022
     0.22005156 0.24091195 0.21168044 0.15238867 0.60493878 0.32644935
     0.26125213 0.07517093 0.13893026 0.74034636 0.84746075 0.45213182
     0.0706292  0.25942968 0.22005156 0.01835698 0.14163263 0.20211369
     0.15238867 0.09990237 0.23918546 0.73072611 0.26215016 0.03608545
     0.03870124 0.16253688 0.74034636 0.97993672 0.08170611 0.64073592
     0.84033393 0.85210036 0.80983396 0.97257783 0.63663757 0.01819022
     0.04521358 0.11500215 0.35283318 0.0604244  0.80983396 0.65427173
     0.56441882 0.21090587 0.09020182 0.15238867 0.09205769 0.13258298
     0.07032472 0.10443874 0.67329436 0.91047691 0.87141113 0.13258298
     0.13893026 0.69001575 0.9854175  0.74034636 0.95157309 0.09990237
     0.97884484 0.51066947 0.04441775 0.04441775 0.28361352 0.03487023
     0.49488971 0.1178021  0.64073592 0.62512052 0.97884484 0.0706292
     0.50493039 0.62403068 0.86836852 0.13893026 0.17455761 0.3031159
     0.07773719 0.37901167 0.11778441 0.4701259  0.40262288 0.9369219
     0.17455761 0.16689812 0.66640667 0.87338811 0.24261599 0.58525135
     0.76060241 0.09058398 0.958343   0.72981059 0.30511879 0.29180956
     0.77425595 0.96775535 0.0858588  0.86836852 0.03084541 0.71900957
     0.08726302 0.05295266 0.34866263 0.32853374 0.034404   0.15950977
     0.91085503 0.52533827 0.80136124 0.55222273 0.07394554 0.24917023
     0.76475846 0.73431446 0.27182894 0.8976234  0.67329436 0.04441775
     0.30124969 0.97648392 0.16253688 0.14892722 0.02069282 0.28267012
     0.05742461 0.05012194 0.12648308 0.06745077 0.08275843 0.09020182
     0.067159  ]




.. GENERATED FROM PYTHON SOURCE LINES 144-145

We can also generate an ROC curve and calculate tha AUC value. On this dataset, the AUC is 0.817, which is quite close to 1.

.. GENERATED FROM PYTHON SOURCE LINES 145-156

.. code-block:: default


    from sklearn.metrics import roc_curve, auc
    import matplotlib.pyplot as plt

    fpr, tpr, _ = roc_curve(test_y, fitted_p)
    plt.plot(fpr, tpr)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.show()

    print('AUC: ', auc(fpr, tpr))




.. image-sg:: /auto_gallery/images/sphx_glr_plot_logi_and_multiclass_001.png
   :alt: plot logi and multiclass
   :srcset: /auto_gallery/images/sphx_glr_plot_logi_and_multiclass_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    AUC:  0.8344691806754506




.. GENERATED FROM PYTHON SOURCE LINES 157-184

Extension: Multi-class Classification
------------------------------------------- 
Multinomial logistic regression
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
When the number of classes is more than 2, we call it multi-class classification task. Logistic regression can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc. Each object being detected in the image would be assigned a probability between 0 and 1, with a sum of one. The extended model is multinomial logistic regression.

To arrive at the multinomial logistic model, one can imagine, for :math:`K` possible classes, running :math:`K−1` independent logistic regression models, in which one class is chosen as a "pivot" and then the other :math:`K−1` classes are separately regressed against the pivot outcome. This would proceed as follows, if class K (the last outcome) is chosen as the pivot:

..math::
    \ln (\mathbb{P}(y=1)/\mathbb{P}(y=K)) = x^T\beta^{(1)},\\
    \dots\ \dots\\
    \ln (\mathbb{P}(y=K-1)/\mathbb{P}(y=K)) = x^T\beta^{(K-1)}.


Then, the probability to choose the j-th class can be easily derived to be:

..math::
    \mathbb{P}(y=j) = \frac{\exp(x^T\beta^{(j)})}{1+\sum_{k=1}^{K-1} \exp(x^T\beta^{(k)})},


and subsequently, we would predict the :math:`j^*`-th class if the :math:`j^*=\arg\max_j \mathbb{P}(y=j)`. Notice that, for :math:`K` possible classes case, there are :math:`p\times(K−1)` unknown parameters: :math:`\beta^{(1)},\dots,\beta^{(K−1)}` to be estimated. Because the number of parameters increase as :math:`K`, it is even more urge to constrain the model complexity. And the best subset selection for multinomial logistic regression aims to maximize the log-likelihood function and control the model complexity by restricting :math:`B=(\beta^{(1)},\dots,\beta^{(K−1)})` with :math:`||B||_{0,2}\leq s` where :math:`||B||_{0,2}=\sum_{i=1}^p I(B_{i\cdot}=0)`, :math:`B_{i\cdot}` is the :math:`i`-th row of coefficient matrix :math:`B` and :math:`0\in R^{K-1}` is an all zero vector. In other words, each row of :math:`B` would be either all zero or all non-zero.

### Simulated Data Example

We shall conduct Multinomial logistic regression on an artificial dataset for demonstration. The `make_multivariate_glm_data()` provides a simple way to generate suitable for this task. 

The assumption behind is the response vector following a multinomial distribution. The artifical dataset contain 100 observations and 20 predictors but only five predictors have influence on the three possible classes.

.. GENERATED FROM PYTHON SOURCE LINES 184-198

.. code-block:: default




    from abess.datasets import make_multivariate_glm_data
    n = 100 # sample size
    p = 20  # all predictors
    k = 5   # real predictors
    M = 3   # number of classes

    np.random.seed(0)
    dt =  make_multivariate_glm_data(n = n, p = p, k = k, family = "multinomial", M = M)
    print(dt.coef_)
    print('real variables\' index:\n', set(np.nonzero(dt.coef_)[0]))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [[ 0.          0.          0.        ]
     [ 0.          0.          0.        ]
     [ 1.09734231  4.03598978  0.        ]
     [ 0.          0.          0.        ]
     [ 0.          0.          0.        ]
     [ 9.91227834 -3.47987303  0.        ]
     [ 0.          0.          0.        ]
     [ 0.          0.          0.        ]
     [ 0.          0.          0.        ]
     [ 0.          0.          0.        ]
     [ 8.93282229  8.93249765  0.        ]
     [-4.03426165 -2.70336848  0.        ]
     [ 0.          0.          0.        ]
     [ 0.          0.          0.        ]
     [ 0.          0.          0.        ]
     [ 0.          0.          0.        ]
     [ 0.          0.          0.        ]
     [ 0.          0.          0.        ]
     [-5.53475149 -2.65928982  0.        ]
     [ 0.          0.          0.        ]]
    real variables' index:
     {2, 5, 10, 11, 18}




.. GENERATED FROM PYTHON SOURCE LINES 199-200

To carry out best subset selection for multinomial logistic regression, we can call the `MultinomialRegression()`. Here is an example.

.. GENERATED FROM PYTHON SOURCE LINES 200-208

.. code-block:: default




    from abess.linear import MultinomialRegression
    s = 5
    model = MultinomialRegression(support_size = range(0, s + 1))
    model.fit(dt.x, dt.y)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    MultinomialRegression(always_select=[], support_size=range(0, 6))



.. GENERATED FROM PYTHON SOURCE LINES 209-210

Its use is quite similar to `LogisticRegression`. We can get the coefficients to recognize "in-model" variables.

.. GENERATED FROM PYTHON SOURCE LINES 210-216

.. code-block:: default




    print('intercept:\n', model.intercept_)
    print('coefficients:\n', model.coef_)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    intercept:
     [-14.23568599 -13.26313814 -12.29986506]
    coefficients:
     [[  0.           0.           0.        ]
     [  0.           0.           0.        ]
     [ -1.37009824   2.48622361  -0.93411504]
     [  0.           0.           0.        ]
     [  0.           0.           0.        ]
     [-64.65747381 -84.41818076 -79.71885036]
     [  0.           0.           0.        ]
     [  0.           0.           0.        ]
     [  0.           0.           0.        ]
     [  0.           0.           0.        ]
     [  3.75446429   3.27954195  -6.55614709]
     [ -2.41731192  -0.26336353   2.52314973]
     [  0.           0.           0.        ]
     [  0.           0.           0.        ]
     [  0.           0.           0.        ]
     [  0.           0.           0.        ]
     [  0.           0.           0.        ]
     [  0.           0.           0.        ]
     [ -4.32041106   1.34674771   2.69302205]
     [  0.           0.           0.        ]]




.. GENERATED FROM PYTHON SOURCE LINES 217-218

So those variables used in model can be recognized and we ca find that they are the same as the data's "real" coefficients we generate.

.. GENERATED FROM PYTHON SOURCE LINES 218-223

.. code-block:: default




    print('used variables\' index:\n', set(np.nonzero(model.coef_)[0]))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    used variables' index:
     {2, 5, 10, 11, 18}




.. GENERATED FROM PYTHON SOURCE LINES 224-227

R tutorial
---------------
For R tutorial, please view [https://abess-team.github.io/abess/articles/v03-classification.html](https://abess-team.github.io/abess/articles/v03-classification.html).


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.249 seconds)


.. _sphx_glr_download_auto_gallery_plot_logi_and_multiclass.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_logi_and_multiclass.py <plot_logi_and_multiclass.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_logi_and_multiclass.ipynb <plot_logi_and_multiclass.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
